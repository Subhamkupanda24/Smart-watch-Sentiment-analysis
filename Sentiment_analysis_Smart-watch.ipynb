{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment anaysis on \"Noise colourfit watch\" reviews in amazon\n",
    "\n",
    "import requests   # Importing requests to extract content from a url\n",
    "from bs4 import BeautifulSoup as bs # Beautifulsoup is for web scrapping...used to scrap specific content \n",
    "import re \n",
    "\n",
    "#pip install wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# creating empty reviews list \n",
    "noise_reviews=[]\n",
    "\n",
    "\n",
    "for i in range(1,21):\n",
    "  ip=[]  \n",
    "  url=\"https://www.amazon.in/Noise-ColorFit-Monitor-Personalised-Waterproof/product-reviews/B08PZ96F8V/ref=cm_cr_getr_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=\"+str(i)\n",
    "  response = requests.get(url)\n",
    "  soup = bs(response.content,\"html.parser\")# creating soup object to iterate over the extracted content \n",
    "  reviews = soup.find_all(\"span\",attrs={\"class\",\"a-size-base review-text review-text-content\"})# Extracting the content under specific tags  \n",
    "  for i in range(len(reviews)):\n",
    "    ip.append(reviews[i].text)  \n",
    " \n",
    "  noise_reviews=noise_reviews+ip  # adding the reviews of one page to empty list which in future contains all the reviews\n",
    "\n",
    "# writng reviews in a text file \n",
    "with open(\"noise_py.txt\",\"w\",encoding='utf8') as output:\n",
    "    output.write(str(noise_reviews))\n",
    "\t\n",
    "\n",
    "# Joinining all the reviews into single paragraph \n",
    "ip_rev_string = \" \".join(noise_reviews)\n",
    "\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Removing unwanted symbols incase if exists\n",
    "ip_rev_string = re.sub(\"[^A-Za-z\" \"]+\",\" \", ip_rev_string).lower()\n",
    "ip_rev_string = re.sub(\"[0-9\" \"]+\",\" \", ip_rev_string)\n",
    "\n",
    "# words that contained in iphone XR reviews\n",
    "ip_reviews_words = ip_rev_string.split(\" \")\n",
    "\n",
    "#TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ip_reviews_words, use_idf=True,ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(ip_reviews_words)\n",
    "\n",
    "with open(\"C:/Users/USER/Desktop/Data Mining/Text_Mining/stop.txt\",\"r\") as sw:\n",
    "    stop_words = sw.read()\n",
    "    \n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "\n",
    "stop_words.extend([\"noise\",\"watch\",\"product\",\"good\",\"app\"])\n",
    "\n",
    "ip_reviews_words = [w for w in ip_reviews_words if not w in stop_words]\n",
    "\n",
    "# Joinining all the reviews into single paragraph \n",
    "ip_rev_string = \" \".join(ip_reviews_words)\n",
    "\n",
    "# WordCloud can be performed on the string inputs.\n",
    "# Corpus level word cloud\n",
    "\n",
    "wordcloud_ip = WordCloud(\n",
    "                      background_color='White',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(ip_rev_string)\n",
    "\n",
    "plt.imshow(wordcloud_ip)\n",
    "\n",
    "# positive words # Choose the path for +ve words stored in system\n",
    "with open(\"C:/Users/USER/Desktop/Data Mining/Text_Mining/positive-words.txt\",\"r\") as pos:\n",
    "  poswords = pos.read().split(\"\\n\")\n",
    "\n",
    "# Positive word cloud\n",
    "# Choosing the only words which are present in positive words\n",
    "ip_pos_in_pos = \" \".join ([w for w in ip_reviews_words if w in poswords])\n",
    "\n",
    "wordcloud_pos_in_pos = WordCloud(\n",
    "                      background_color='White',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(ip_pos_in_pos)\n",
    "plt.figure(2)\n",
    "plt.imshow(wordcloud_pos_in_pos)\n",
    "\n",
    "# negative words Choose path for -ve words stored in system\n",
    "with open(\"C:/Users/USER/Desktop/Data Mining/Text_Mining/negative-words.txt\", \"r\") as neg:\n",
    "  negwords = neg.read().split(\"\\n\")\n",
    "\n",
    "# negative word cloud\n",
    "# Choosing the only words which are present in negwords\n",
    "ip_neg_in_neg = \" \".join ([w for w in ip_reviews_words if w in negwords])\n",
    "\n",
    "wordcloud_neg_in_neg = WordCloud(\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(ip_neg_in_neg)\n",
    "plt.figure(3)\n",
    "plt.imshow(wordcloud_neg_in_neg)\n",
    "\n",
    "\n",
    "# wordcloud with bigram\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "WNL = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Lowercase and tokenize\n",
    "text = ip_rev_string.lower()\n",
    "\n",
    "# Remove single quote early since it causes problems with the tokenizer.\n",
    "text = text.replace(\"'\", \"\")\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "text1 = nltk.Text(tokens)\n",
    "\n",
    "# Remove extra chars and remove stop words.\n",
    "text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text1]\n",
    "\n",
    "# Create a set of stopwords\n",
    "stopwords_wc = set(STOPWORDS)\n",
    "customised_words = ['price', 'great'] # If you want to remove any particular word form text which does not contribute much in meaning\n",
    "\n",
    "new_stopwords = stopwords_wc.union(customised_words)\n",
    "\n",
    "# Remove stop words\n",
    "text_content = [word for word in text_content if word not in new_stopwords]\n",
    "\n",
    "# Take only non-empty entries\n",
    "text_content = [s for s in text_content if len(s) != 0]\n",
    "\n",
    "# Best to get the lemmas of each word to reduce the number of similar words\n",
    "text_content = [WNL.lemmatize(t) for t in text_content]\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(text)  \n",
    "bigrams_list = list(nltk.bigrams(text_content))\n",
    "print(bigrams_list)\n",
    "\n",
    "dictionary2 = [' '.join(tup) for tup in bigrams_list]\n",
    "print (dictionary2)\n",
    "\n",
    "# Using count vectoriser to view the frequency of bigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "bag_of_words = vectorizer.fit_transform(dictionary2)\n",
    "vectorizer.vocabulary_\n",
    "\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "print(words_freq[:100])\n",
    "\n",
    "# Generating wordcloud\n",
    "words_dict = dict(words_freq)\n",
    "WC_height = 1000\n",
    "WC_width = 1500\n",
    "WC_max_words = 200\n",
    "wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width, stopwords=new_stopwords)\n",
    "wordCloud.generate_from_frequencies(words_dict)\n",
    "\n",
    "plt.figure(4)\n",
    "plt.title('Most frequently occurring bigrams connected by same colour and font size')\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
